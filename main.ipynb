{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "laughing-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from matplotlib import image\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    " \n",
    "plt.rcParams['figure.figsize'] = (5,5)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    " \n",
    "tf.random.set_seed(42)\n",
    "np.random.RandomState(42)\n",
    " \n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-three",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "crazy-benchmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6758/6758 [00:31<00:00, 217.59it/s]\n",
      "100%|██████████| 6646/6646 [00:29<00:00, 224.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from helper import load_data, crop_images\n",
    "ER, yER = load_data('train/ER/')\n",
    "NR, yNR = load_data('train/NR/')\n",
    "\n",
    "ER = crop_images(ER)\n",
    "NR = crop_images(NR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quarterly-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypER = np.zeros(ER.shape[0])\n",
    "ypNR = np.ones(NR.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "junior-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest, yptrain, yptest = train_test_split(np.concatenate([ER, NR], axis=0), \n",
    "                                                np.concatenate([yER, yNR], axis=0), \n",
    "                                                np.concatenate([ypER, ypNR], axis=0), shuffle=True, \n",
    "                                               random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "shared-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "Xtrain = scaler.fit_transform(Xtrain.reshape(-1, 1)).reshape(-1, Xtrain.shape[1], Xtrain.shape[2], 1)\n",
    "Xtest = scaler.transform(Xtest.reshape(-1, 1)).reshape(-1, Xtrain.shape[1], Xtrain.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "marked-beverage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10053, 126, 126, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-faculty",
   "metadata": {},
   "source": [
    "# CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compound-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(Xtrain.shape[1], Xtrain.shape[2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sporting-breed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Conv2D(30, 3, activation='relu')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPool2D()(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "x1 = layers.Dense(100,activation='relu')(x)\n",
    "x1 = layers.Dropout(0.1)(x1)\n",
    "\n",
    "x2 = layers.Dense(300,activation='relu')(x)\n",
    "x2 = layers.Dropout(0.2)(x2)\n",
    "\n",
    "ptype = layers.Dense(1, activation = \"sigmoid\", name='ptype')(x1)\n",
    "energy = layers.Dense(1, name='energy')(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cardiac-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs={'en': energy, 'pty':ptype})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "standard-middle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 126, 126, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 124, 124, 30) 300         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 124, 124, 30) 120         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 62, 62, 30)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 115320)       0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 115320)       0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          34596300    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          11532100    dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 300)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "energy (Dense)                  (None, 1)            301         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ptype (Dense)                   (None, 1)            101         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 46,129,222\n",
      "Trainable params: 46,129,162\n",
      "Non-trainable params: 60\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "static-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "        loss={'en':'mae','pty': tf.losses.BinaryCrossentropy(from_logits = False)},\n",
    "        loss_weights={'en':1, 'pty':0.1})\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='energy_loss', patience = 30)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "        \"MLHEP_v1/weights.{epoch:02d}-{energy_loss:.4f}.hdf5\", \n",
    "        monitor='energy_loss', verbose = 1, save_best_only = True, \n",
    "        mode='auto', save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "neither-period",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "315/315 [==============================] - 89s 279ms/step - loss: 4.7281 - energy_loss: 4.6624 - ptype_loss: 0.6564 - val_loss: 7.0373 - val_energy_loss: 6.9667 - val_ptype_loss: 0.7060\n",
      "\n",
      "Epoch 00001: energy_loss improved from inf to 3.53141, saving model to MLHEP_v1/weights.01-3.5314.hdf5\n",
      "Epoch 2/10\n",
      "315/315 [==============================] - 86s 274ms/step - loss: 2.4086 - energy_loss: 2.3614 - ptype_loss: 0.4727 - val_loss: 4.4848 - val_energy_loss: 4.3972 - val_ptype_loss: 0.8758\n",
      "\n",
      "Epoch 00002: energy_loss improved from 3.53141 to 2.39912, saving model to MLHEP_v1/weights.02-2.3991.hdf5\n",
      "Epoch 3/10\n",
      "315/315 [==============================] - 86s 274ms/step - loss: 2.1347 - energy_loss: 2.1042 - ptype_loss: 0.3044 - val_loss: 6.1660 - val_energy_loss: 6.1262 - val_ptype_loss: 0.3979\n",
      "\n",
      "Epoch 00003: energy_loss improved from 2.39912 to 2.06341, saving model to MLHEP_v1/weights.03-2.0634.hdf5\n",
      "Epoch 4/10\n",
      "315/315 [==============================] - 88s 280ms/step - loss: 2.0528 - energy_loss: 2.0338 - ptype_loss: 0.1905 - val_loss: 1.7924 - val_energy_loss: 1.7775 - val_ptype_loss: 0.1493\n",
      "\n",
      "Epoch 00004: energy_loss improved from 2.06341 to 1.96500, saving model to MLHEP_v1/weights.04-1.9650.hdf5\n",
      "Epoch 5/10\n",
      "315/315 [==============================] - 88s 278ms/step - loss: 1.8175 - energy_loss: 1.8047 - ptype_loss: 0.1279 - val_loss: 1.6740 - val_energy_loss: 1.6634 - val_ptype_loss: 0.1057\n",
      "\n",
      "Epoch 00005: energy_loss improved from 1.96500 to 1.77155, saving model to MLHEP_v1/weights.05-1.7716.hdf5\n",
      "Epoch 6/10\n",
      "315/315 [==============================] - 86s 274ms/step - loss: 1.8299 - energy_loss: 1.8208 - ptype_loss: 0.0908 - val_loss: 1.6699 - val_energy_loss: 1.6616 - val_ptype_loss: 0.0827\n",
      "\n",
      "Epoch 00006: energy_loss improved from 1.77155 to 1.75259, saving model to MLHEP_v1/weights.06-1.7526.hdf5\n",
      "Epoch 7/10\n",
      "315/315 [==============================] - 87s 275ms/step - loss: 1.7183 - energy_loss: 1.7110 - ptype_loss: 0.0733 - val_loss: 2.8677 - val_energy_loss: 2.8592 - val_ptype_loss: 0.0856\n",
      "\n",
      "Epoch 00007: energy_loss improved from 1.75259 to 1.67814, saving model to MLHEP_v1/weights.07-1.6781.hdf5\n",
      "Epoch 8/10\n",
      "315/315 [==============================] - 86s 272ms/step - loss: 1.6889 - energy_loss: 1.6837 - ptype_loss: 0.0522 - val_loss: 1.5900 - val_energy_loss: 1.5838 - val_ptype_loss: 0.0612\n",
      "\n",
      "Epoch 00008: energy_loss improved from 1.67814 to 1.61879, saving model to MLHEP_v1/weights.08-1.6188.hdf5\n",
      "Epoch 9/10\n",
      "315/315 [==============================] - 86s 273ms/step - loss: 1.5790 - energy_loss: 1.5746 - ptype_loss: 0.0444 - val_loss: 1.6000 - val_energy_loss: 1.5959 - val_ptype_loss: 0.0414\n",
      "\n",
      "Epoch 00009: energy_loss improved from 1.61879 to 1.58892, saving model to MLHEP_v1/weights.09-1.5889.hdf5\n",
      "Epoch 10/10\n",
      "315/315 [==============================] - 83s 263ms/step - loss: 1.5736 - energy_loss: 1.5700 - ptype_loss: 0.0359 - val_loss: 2.0380 - val_energy_loss: 2.0334 - val_ptype_loss: 0.0453\n",
      "\n",
      "Epoch 00010: energy_loss improved from 1.58892 to 1.57083, saving model to MLHEP_v1/weights.10-1.5708.hdf5\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        Xtrain, {'en':ytrain, 'pty':yptrain}, \n",
    "        validation_data = (Xtest, {'en':ytest, 'pty':yptest}),\n",
    "        epochs = 10, batch_size = 32, shuffle = True,\n",
    "        callbacks=[callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-handbook",
   "metadata": {},
   "source": [
    "# Random forest predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "broadband-biodiversity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('MLHEP_v1/bestv9.230-0.7786.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "living-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_predictions_test = model.predict(Xtest)\n",
    "\n",
    "CNN_predictions_train = model.predict(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "united-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.stack(\n",
    "        [np.squeeze(CNN_predictions_train['en']), np.squeeze(Xtrain.sum(axis=(1,2)))], axis=1)\n",
    "\n",
    "features_test = np.stack(\n",
    "        [np.squeeze(CNN_predictions_test['en']), np.squeeze(Xtest.sum(axis=(1,2)))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acting-laptop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(criterion='mae', max_depth=50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_forest = RandomForestRegressor(criterion='mae', max_depth=50)\n",
    "model_forest.fit(features_train, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daily-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forest = model_forest.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-frame",
   "metadata": {},
   "source": [
    "# Create submission file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "heavy-delay",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16560/16560 [01:10<00:00, 233.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from helper import load_test\n",
    "\n",
    "data, n_id = load_test('test/pattern/')\n",
    "data = crop_images(data)\n",
    "data = scaler.transform(data.reshape(-1, 1)).reshape(-1,126,126,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "narrative-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(data)\n",
    "\n",
    "feature_input = np.stack(\n",
    "    [np.squeeze(predictions['en']), np.squeeze(data.sum(axis=(1,2)))], axis=1)\n",
    "\n",
    "forest_predictions = model_forest.predict(feature_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "complimentary-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "d = {'id': n_id, 'energy': forest_predictions}\n",
    "dataframe = pd.DataFrame(d)\n",
    "\n",
    "dataframe.to_csv('test_file', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
